import requests
import re
import yaml
import datetime
import urllib3
import os

# ç¦ç”¨ä¸å®‰å…¨çš„è¯·æ±‚è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


github_urls = [
    "https://github.com/abshare/abshare.github.io/blob/main/README.md",
    "https://github.com/mkshare3/mkshare3.github.io/blob/main/README.md",
    "https://github.com/toshare5/toshare5.github.io/blob/main/README.md",
    "https://github.com/abshare3/abshare3.github.io/blob/main/README.md",
    "https://github.com/tolinkshare2/tolinkshare2.github.io/blob/main/README.md",
    "https://github.com/mksshare/mksshare.github.io/blob/main/README.md"
]

CLASH_USER_AGENT = "clash-verge/v2.4.4"
TEMPLATE_FILE = "template.yaml"
OUTPUT_FILE = "merged_config.yaml"
CACHE_FILE = "cache.yaml"


def get_raw_url(github_url):
    """
    å°† GitHub blob URL è½¬æ¢ä¸º raw URLï¼Œæ–¹ä¾¿ä¸‹è½½ã€‚
    ï¼ˆæ³¨ï¼šä¸ºäº†åœ¨å¤§é™†åœ°åŒºè®¿é—®ï¼Œå¯ä»¥åˆ‡æ¢ä¸º gh.llkk.cc ç­‰åŠ é€ŸæœåŠ¡ï¼‰
    """
    # return "https://gh.llkk.cc/" + github_url
    return github_url.replace("github.com", "raw.githubusercontent.com").replace("/blob/", "/")


def get_subscription_link(github_url):
    """ä» GitHub README ä¸­æå– Clash è®¢é˜…é“¾æ¥ã€‚"""
    raw_url = get_raw_url(github_url)
    print(f"-> å°è¯•ä» {raw_url} è·å–è®¢é˜…é“¾æ¥...")
    try:
        response = requests.get(raw_url, timeout=10, verify=False)
        response.raise_for_status()
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…â€œClashè®¢é˜…é“¾æ¥â€åçš„ç¬¬ä¸€ä¸ª http/https é“¾æ¥ï¼Œre.S æ ‡å¿—è®© '.' åŒ¹é…åŒ…æ‹¬æ¢è¡Œç¬¦åœ¨å†…çš„æ‰€æœ‰å­—ç¬¦
        match = re.search(r'Clashè®¢é˜…é“¾æ¥.*?((?:https?|http)://\S+)', response.text, re.S)
        if match:
            link = match.group(1).strip()
            print(f"   âœ“ æˆåŠŸæ‰¾åˆ°é“¾æ¥: {link}")
            return link
        else:
            print(f"   âœ— åœ¨ {raw_url} ä¸­æœªæ‰¾åˆ° Clash è®¢é˜…é“¾æ¥ã€‚")
            return None
    except requests.exceptions.RequestException as e:
        print(f"   âœ— è¯·æ±‚ {raw_url} å¤±è´¥: {e}")
        return None
    

def get_current_ip():
    """è·å–å½“å‰ç½‘ç»œçš„å‡ºå£ IP åœ°å€ã€‚"""
    print("--- æ£€æŸ¥ç½‘ç»œè¿æ¥ ---")
    try:
        response = requests.get('https://api.ipify.org?format=json', timeout=5)
        print(f"å½“å‰è®¿é—® IP: {response.json().get('ip')}")
    except requests.exceptions.RequestException as e:
        print(f"æ— æ³•è·å–å½“å‰ IPï¼Œç½‘ç»œå¯èƒ½å­˜åœ¨é—®é¢˜: {e}")
    print("-" * 31)


def download_and_extract_proxies(link):
    """ä¸‹è½½å¹¶è§£æ Clash é…ç½®æ–‡ä»¶ï¼Œæå–å¹¶è¿‡æ»¤ä»£ç†èŠ‚ç‚¹åˆ—è¡¨ã€‚"""
    try:
        headers = {'User-Agent': CLASH_USER_AGENT}
        response = requests.get(link, headers=headers, timeout=20, verify=False)
        response.raise_for_status()
        config_data = yaml.safe_load(response.text)
        if not config_data or 'proxies' not in config_data:
            print(f" Â  âœ— è­¦å‘Š: YAML è§£æä¸ºç©ºæˆ–ç¼ºå°‘ 'proxies' éƒ¨åˆ†, é“¾æ¥: {link}")
            return None
        
        proxies = config_data.get('proxies', [])
        filtered_proxies = [
            p for p in proxies
            if isinstance(p, dict) and 'name' in p and "å‰©ä½™æµé‡" not in p['name'] and "å¥—é¤åˆ°æœŸ" not in p['name']
        ]
        return filtered_proxies
        
    except (requests.exceptions.RequestException, yaml.YAMLError) as e:
        print(f" Â  âœ— ä¸‹è½½æˆ–è§£æ {link} å¤±è´¥: {e}")
        return None

def load_cache():
    """ä¼˜å…ˆä»è¿œç¨‹ cache.yaml åŠ è½½æ•°æ®ï¼Œå¤±è´¥åˆ™å›é€€æœ¬åœ°ç¼“å­˜ã€‚"""
    remote_url = "https://raw.githubusercontent.com/lkchx123/get_nodes/main/cache.yaml"
    try:
        print(f"-> å°è¯•ä»è¿œç¨‹åŠ è½½ç¼“å­˜: {remote_url}")
        response = requests.get(remote_url, timeout=10)
        response.raise_for_status()
        print(f"âœ“ æˆåŠŸåŠ è½½è¿œç¨‹ç¼“å­˜æ–‡ä»¶")
        return yaml.safe_load(response.text) or {}
    except Exception as e:
        print(f"âœ— è­¦å‘Š: åŠ è½½è¿œç¨‹ç¼“å­˜å¤±è´¥: {e}")
        if os.path.exists(CACHE_FILE):
            try:
                with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                    print(f"âœ“ æˆåŠŸåŠ è½½æœ¬åœ°ç¼“å­˜æ–‡ä»¶ '{CACHE_FILE}'")
                    return yaml.safe_load(f) or {}
            except Exception as e:
                print(f"âœ— è­¦å‘Š: åŠ è½½æœ¬åœ°ç¼“å­˜æ–‡ä»¶ '{CACHE_FILE}' å¤±è´¥: {e}")
    return {}

def save_cache(cache_data):
    """å°†æ•°æ®ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶ã€‚"""
    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            yaml.dump(cache_data, f, sort_keys=False, allow_unicode=True)
        print(f"âœ“ æˆåŠŸæ›´æ–°ç¼“å­˜æ–‡ä»¶ '{CACHE_FILE}'")
    except Exception as e:
        print(f"âœ— è­¦å‘Š: ä¿å­˜ç¼“å­˜æ–‡ä»¶ '{CACHE_FILE}' å¤±è´¥: {e}")


def merge_configs():
    """
    ä¸»å‡½æ•°ï¼šåŠ è½½æ¨¡æ¿ï¼Œä¸‹è½½å¹¶è¿½åŠ æ‰€æœ‰æ–°ä»£ç†èŠ‚ç‚¹ï¼Œç„¶åæ›´æ–°æ¨¡æ¿ä¸­çš„ä»£ç†ç»„ã€‚
    """
    get_current_ip()

    # 1. åŠ è½½æœ¬åœ°æ¨¡æ¿æ–‡ä»¶
    try:
        with open(TEMPLATE_FILE, 'r', encoding='utf-8') as f:
            base_config = yaml.safe_load(f)
        print(f"\nâœ“ æˆåŠŸåŠ è½½æ¨¡æ¿æ–‡ä»¶ '{TEMPLATE_FILE}'")
    except Exception as e:
        print(f"\nâœ— é”™è¯¯: åŠ è½½æˆ–è§£ææ¨¡æ¿æ–‡ä»¶ '{TEMPLATE_FILE}' å¤±è´¥: {e}")
        return

    # åŠ è½½ç¼“å­˜
    cache = load_cache()

    print("\n--- å¼€å§‹è·å–å¹¶åˆå¹¶ Clash è®¢é˜… ---")

    all_new_proxies = []
    success_count = 0

    # 2. éå†URLï¼Œè·å–æ‰€æœ‰ä»£ç†èŠ‚ç‚¹
    for url in github_urls:
        author_match = re.search(r'github\.com/([a-zA-Z0-9_-]+)/', url)
        author = author_match.group(1) if author_match else "unknown"

        # ä» GitHub README ä¸­è·å–è®¢é˜…é“¾æ¥
        current_clash_link = get_subscription_link(url)

        proxies_from_sub = None

        # 3. æ£€æŸ¥ç¼“å­˜
        if url in cache and cache[url].get('clash_link') == current_clash_link:
            # æ‰¾åˆ°ç¼“å­˜ï¼Œä¸”clashé“¾æ¥æ²¡å˜
            print(f"   âœ“ è®¢é˜…é“¾æ¥æœªå˜ï¼Œä½¿ç”¨ç¼“å­˜ã€‚")
            proxies_from_sub = cache[url].get('proxies')

        # 4. å¦‚æœæ²¡æœ‰å‘½ä¸­ç¼“å­˜æˆ–è€…ç¼“å­˜èŠ‚ç‚¹æ— æ•ˆï¼Œåˆ™é‡æ–°ä¸‹è½½
        if not proxies_from_sub:
            if not current_clash_link:
                print("-" * 31)
                # è·å–è®¢é˜…é“¾æ¥å¤±è´¥ï¼Œä»ç¼“å­˜ä¸­åˆ é™¤æ­¤é¡¹ï¼Œé˜²æ­¢æ®‹ç•™
                if url in cache:
                    print(f"   âœ— è·å–æ–°é“¾æ¥å¤±è´¥ï¼Œåˆ é™¤æ—§ç¼“å­˜ã€‚")
                    del cache[url]
                continue

            proxies_from_sub = download_and_extract_proxies(current_clash_link)

            if proxies_from_sub:
                # ä¸‹è½½æˆåŠŸï¼Œæ›´æ–°ç¼“å­˜
                cache[url] = {
                    'clash_link': current_clash_link,
                    'proxies': proxies_from_sub
                }
            else:
                # ä¸‹è½½å¤±è´¥ï¼Œä»ç¼“å­˜ä¸­åˆ é™¤æ­¤é¡¹ï¼Œé˜²æ­¢æ®‹ç•™
                if url in cache:
                    print(f"   âœ— ä¸‹è½½å¤±è´¥ï¼Œåˆ é™¤æ—§ç¼“å­˜ã€‚")
                    del cache[url]

        if proxies_from_sub:
            for proxy in proxies_from_sub:
                if isinstance(proxy, dict) and 'name' in proxy:
                    new_proxy = proxy.copy()  # åˆ›å»ºä¸€ä¸ªæµ…æ‹·è´å‰¯æœ¬ï¼Œé˜²æ­¢ç¼“å­˜ä¸­å¸¦authorï¼Œä»è€Œé‡å¤æ·»åŠ author
                    new_proxy['name'] = f"{new_proxy['name']} | {author}"
                    all_new_proxies.append(new_proxy)
            print(f"   âœ“ ä»æ­¤è®¢é˜…ä¸­è·å–äº† {len(proxies_from_sub)} ä¸ªä»£ç†èŠ‚ç‚¹ã€‚")
            success_count += 1
        print("-" * 31)

    # ä¿å­˜æ›´æ–°åçš„ç¼“å­˜
    save_cache(cache)

    if success_count == 0:
        print("\nâœ— æ‰€æœ‰è®¢é˜…é“¾æ¥å‡æ— æ³•è·å–")

    # 5. ä¿ç•™æ¨¡æ¿åŸæœ‰èŠ‚ç‚¹ï¼Œå°†æ–°èŠ‚ç‚¹è¿½åŠ åˆ°åé¢
    new_proxy_names = [p['name'] for p in all_new_proxies]
    original_proxies = base_config['proxies']
    base_config['proxies'] = original_proxies + all_new_proxies

    # 6. ç›´æ¥æŸ¥æ‰¾'æ‰‹åŠ¨é€‰æ‹©'ä»£ç†ç»„å¹¶è¿½åŠ æ–°èŠ‚ç‚¹åç§°
    if 'proxy-groups' in base_config and base_config['proxy-groups']:
        anchor_group_name = "æ‰‹åŠ¨é€‰æ‹©"
        for group in base_config['proxy-groups']:
            if group.get('name') == anchor_group_name:
                group['proxies'].extend(new_proxy_names)
                break
    
    # 7. å†™å…¥æœ€ç»ˆçš„åˆå¹¶é…ç½®
    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    timestamp = timestamp.translate(str.maketrans('0123456789', 'ğŸ¬ğŸ­ğŸ®ğŸ¯ğŸ°ğŸ±ğŸ²ğŸ³ğŸ´ğŸµ'))
    yaml_content = yaml.dump(base_config, sort_keys=False, allow_unicode=True)
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write(f"# â°é…ç½®åˆå¹¶äº {timestamp}\n")
        f.write(yaml_content.replace("æµ‹è¯•èŠ‚ç‚¹", f"â°â°â°é…ç½®åˆå¹¶äº {timestamp}"))

    print("\n--- åˆå¹¶å®Œæˆï¼ ---")
    print(f"å·²ä»{success_count}ä¸ªè®¢é˜…é“¾æ¥ä¸­æˆåŠŸåˆå¹¶äº† {len(new_proxy_names)} ä¸ªèŠ‚ç‚¹ã€‚")
    print(f"æœ€ç»ˆé…ç½®æ–‡ä»¶ '{OUTPUT_FILE}' å·²ç”Ÿæˆã€‚")


if __name__ == "__main__":
    merge_configs()








